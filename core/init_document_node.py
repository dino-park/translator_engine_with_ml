# core/init_document_node.py
# Document, Node ìƒì„± ë° ìºì‹œ ê´€ë¦¬

import os
import pandas as pd
import pickle
import hashlib
from tqdm import tqdm

from llama_index.core import Document

from core.normalizer import (
    to_embedding_normalize_for_search,
    split_text_segments,
    split_segments_paired,
)
from core.config import Settings
from utils import get_translator_logger


logger = get_translator_logger("core.init_document_node")


def generate_id_from_text(text: str) -> str:
    """
    Textì—ì„œ ê³ ìœ  ID ìƒì„±(Hash ê¸°ë°˜)
    - ë™ì¼ í…ìŠ¤íŠ¸ëŠ” í•­ìƒ ë™ì¼ ID ë°˜í™˜
    - ì¶©ëŒ ì—†ì´ ì¦ë¶„ Indexing ê°€ëŠ¥

    Args:
        text (str): ê³ ìœ  ID ìƒì„±í•  í…ìŠ¤íŠ¸

    Returns:
        str: 8ìë¦¬ Hash ID
    """
    # SHA256 Hash ì²˜ìŒ 8ìë¦¬ ì‚¬ìš©
    hash_obj = hashlib.sha256(text.encode('utf-8'))
    return hash_obj.hexdigest()[:8]


def load_dataset(csv_path: str) -> pd.DataFrame:
    """
    csv ë¡œë“œ ë° ê¸°ë³¸ ì •ë¦¬
    - í•„ìˆ˜ ì»¬ëŸ¼: cn, ko

    Args:
        csv_path (str): íŒŒì¼ ê²½ë¡œ

    Returns:
        pd.DataFrame: 
    """
    
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"CSV not found: {csv_path}")
    # ----- ì»¬ëŸ¼ëª… ì†Œë¬¸ìë¡œ í†µì¼ -----
    df = pd.read_csv(csv_path)
    df.columns = df.columns.str.lower()
    # ----- cn key ê¸°ì¤€ìœ¼ë¡œ, ì¤‘ë³µëœ í–‰ì„ ì œê±°í•˜ê³  ì²« ë²ˆì§¸ í–‰ë§Œ ìœ ì§€
    df = df[["cn", "ko"]].drop_duplicates(subset="cn", keep="first")
    # ----- NaN/ê³µë°± ì •ë¦¬ -----
    df[["cn", "ko"]] = df[["cn", "ko"]].fillna("").astype(str)
    df["cn"] = df["cn"].str.strip()
    df["ko"] = df["ko"].str.strip()
    
    df = df[df["cn"] != ""]
    return df


def build_documents(df: pd.DataFrame) -> tuple[list[Document], list[Document]]:
    """
    CN/KO ë³„ë„ ì»¬ë ‰ì…˜ìš© Documents ìƒì„±
    - ì½˜í…ì¸  ê¸°ë°˜ ID ìƒì„± (Hash) -> ì¦ë¶„ Indexing ì§€ì›
    - ë©”íƒ€ë°ì´í„°ê°€ ë„ˆë¬´ ê¸´ ê²½ìš°(20ì ì´ˆê³¼) skip
    
    Returns:
        (cn_docs, ko_docs) íŠœí”Œ
        - cn_docs: CN ì»¬ë ‰ì…˜ìš© (cn, cn_normalized, cn_segment)
        - ko_docs: KO ì»¬ë ‰ì…˜ìš© (ko, ko_segment)
    """
    # ë©”íƒ€ë°ì´í„° ìµœëŒ€ ê¸¸ì´ ì œí•œ (chunk_sizeë³´ë‹¤ ì‘ì•„ì•¼ í•¨)
    # cn, ko í…ìŠ¤íŠ¸ê°€ ë©”íƒ€ë°ì´í„°ì˜ ëŒ€ë¶€ë¶„ì„ ì°¨ì§€í•˜ë¯€ë¡œ ì´ë“¤ë§Œ ì²´í¬
    MAX_METADATA_LENGTH = 20  # 20ì ì´ˆê³¼ëŠ” ë¹„ì •ìƒ ë°ì´í„°ë¡œ ê°„ì£¼
    
    cn_docs: list[Document] = []
    ko_docs: list[Document] = []
    skipped_count = 0
    
    for i, row in tqdm(df.iterrows(), total=len(df), desc="ğŸ“„ Documents", unit="rows", ncols=80):
        cn = row["cn"]
        ko = row["ko"]
        
        # ===== ë©”íƒ€ë°ì´í„° ê¸¸ì´ ì‚¬ì „ ì²´í¬ (ë„ˆë¬´ ê¸´ ê²½ìš° skip) =====
        # cn, ko í…ìŠ¤íŠ¸ê°€ ê°œë³„ì ìœ¼ë¡œ ë„ˆë¬´ ê¸¸ë©´ í•´ë‹¹ í–‰ ì „ì²´ skip
        if len(cn) > MAX_METADATA_LENGTH or (ko and len(ko) > MAX_METADATA_LENGTH):
            skipped_count += 1
            logger.warning(
                f"Skipping row {i}: text too long (cn={len(cn)}, ko={len(ko) if ko else 0}, max={MAX_METADATA_LENGTH})"
            )
            continue
        
        # ===== ì½˜í…ì¸  ê¸°ë°˜ ID ìƒì„±(ì¶©ëŒ ë°©ì§€) =====
        base_id = generate_id_from_text(cn)
        entry_id = f"entry-{base_id}"
        
        # ===== CN Documents =====
        # CN ì›ë³¸
        cn_meta = {
            "entry_id": entry_id,
            "cn": cn,
            "doc_type": "cn"
        }
        if ko:
            cn_meta["ko"] = ko
        cn_docs.append(Document(text=cn, doc_id=f"cn-{base_id}", metadata=cn_meta))
        
        # CN normalized
        cn_normalized = to_embedding_normalize_for_search(cn)
        if cn_normalized and cn_normalized != cn:
            cn_normalized_meta = {
                "entry_id": entry_id,
                "cn": cn,
                "doc_type": "cn_normalized"
            }
            if ko:
                cn_normalized_meta["ko"] = ko
            cn_docs.append(Document(
                text=cn_normalized,
                doc_id=f"cn_normalized-{base_id}",
                metadata=cn_normalized_meta
            ))
        
        # ===== KO Documents =====
        if ko:
            ko_meta = {
                "entry_id": entry_id,
                "cn": cn,
                "ko": ko,
                "doc_type": "ko"
            }
            ko_docs.append(Document(text=ko, doc_id=f"ko-{base_id}", metadata=ko_meta))
        
        # ===== Segment Documents =====
        paired_segments = split_segments_paired(cn, ko) if ko else []
        
        if paired_segments:
            for j, (cn_seg, ko_seg) in enumerate(paired_segments):
                if cn_seg == cn:
                    continue
                
                # Segment ê¸¸ì´ ì²´í¬ (20ì ì´ˆê³¼ ì‹œ skip)
                if len(cn_seg) > MAX_METADATA_LENGTH or (ko_seg and len(ko_seg) > MAX_METADATA_LENGTH):
                    logger.debug(f"Skipping segment: cn_seg={len(cn_seg)}, ko_seg={len(ko_seg) if ko_seg else 0}")
                    continue
                
                seg_id = generate_id_from_text(f"{cn}_{cn_seg}_{j}")
                segment_entry_id = f"{entry_id}-seg-{j}"
                
                # CN segment â†’ cn_docs
                cn_seg_meta = {
                    "entry_id": segment_entry_id,
                    "parent_entry_id": entry_id,
                    "cn": cn_seg,
                    "ko": ko_seg,
                    "parent_cn": cn,
                    "parent_ko": ko,
                    "doc_type": "cn_segment",
                    "segment_index": j,
                }
                cn_docs.append(Document(
                    text=cn_seg,
                    doc_id=f"cn_segment-{seg_id}",
                    metadata=cn_seg_meta
                ))
                
                # KO segment â†’ ko_docs
                ko_seg_meta = {
                    "entry_id": segment_entry_id,
                    "parent_entry_id": entry_id,
                    "cn": cn_seg,
                    "ko": ko_seg,
                    "parent_cn": cn,
                    "parent_ko": ko,
                    "doc_type": "ko_segment",
                    "segment_index": j,
                }
                ko_docs.append(Document(
                    text=ko_seg,
                    doc_id=f"ko_segment-{seg_id}",
                    metadata=ko_seg_meta
                ))
        else:
            # ë§¤í•‘ ì‹¤íŒ¨: CN segmentë§Œ ìƒì„±
            segments = split_text_segments(cn)
            for j, segment in enumerate(segments):
                if segment == cn:
                    continue
                
                # Segment ê¸¸ì´ ì²´í¬ (20ì ì´ˆê³¼ ì‹œ skip)
                if len(segment) > MAX_METADATA_LENGTH:
                    logger.debug(f"Skipping CN-only segment: len={len(segment)}")
                    continue
                
                seg_id = generate_id_from_text(f"{cn}_{segment}_{j}")
                segment_entry_id = f"{entry_id}-seg-{j}"
                segment_meta = {
                    "entry_id": segment_entry_id,
                    "parent_entry_id": entry_id,
                    "cn": segment,
                    "parent_cn": cn,
                    "doc_type": "cn_segment",
                    "segment_index": j,
                }
                if ko:
                    segment_meta["parent_ko"] = ko
                
                cn_docs.append(Document(
                    text=segment,
                    doc_id=f"cn_segment-{seg_id}",
                    metadata=segment_meta
                ))
    
    logger.info(
        "Documents built: cn_docs=%d, ko_docs=%d (from %d rows, skipped %d rows with long text)",
        len(cn_docs), len(ko_docs), len(df), skipped_count
    )
    if skipped_count > 0:
        print(f"âš ï¸  {skipped_count}ê°œ í–‰ì´ í…ìŠ¤íŠ¸ ê¸¸ì´ ì´ˆê³¼ë¡œ skipë˜ì—ˆìŠµë‹ˆë‹¤ (ìµœëŒ€: {MAX_METADATA_LENGTH}ì)")
    return cn_docs, ko_docs


# ===============================================
# Node ìƒì„± ë° ìºì‹œ ê´€ë¦¬
# ===============================================

def build_nodes(docs: list[Document]):
    """
    ë¬¸ì„œë¥¼ Node(ê²€ìƒ‰ ë‹¨ìœ„ ë¸”ë¡)ë¡œ ë³€í™˜.
    - chunk_size=512: MAX_METADATA_LENGTH(20ì)ì— ë§ì¶° ì—¬ìœ ìˆê²Œ ì„¤ì •
    - 20ì ì´í•˜ í…ìŠ¤íŠ¸ë§Œ ë“¤ì–´ì˜¤ë¯€ë¡œ 512ë©´ ì¶©ë¶„
    """
    from llama_index.core.node_parser import SentenceSplitter
    parser = SentenceSplitter(
        chunk_size=512,         # 100ì í…ìŠ¤íŠ¸ + ë©”íƒ€ë°ì´í„° ì—¬ìœ ë¶„
        chunk_overlap=50,
    )
    nodes = parser.get_nodes_from_documents(docs)
    logger.info("Nodes built: %d nodes from %d documents", len(nodes), len(docs))
    return nodes


def _get_cache_path(suffix: str = "") -> str:
    """nodes ìºì‹œ íŒŒì¼ ê²½ë¡œ ë°˜í™˜ (ë‚´ë¶€ìš©)"""
    persist_dir = Settings.env.get("PERSIST_DIR", ".")
    filename = f"nodes_cache{suffix}.pkl"
    return os.path.join(persist_dir, filename)


def _save_cache(nodes, suffix: str = "") -> None:
    """nodesë¥¼ pickle íŒŒì¼ë¡œ ì €ì¥ (ë‚´ë¶€ìš©)"""
    cache_path = _get_cache_path(suffix)
    with open(cache_path, "wb") as f:
        pickle.dump(nodes, f)
    logger.info("Nodes cached to %s", cache_path)


def _load_cache(suffix: str = ""):
    """ìºì‹œëœ nodes ë¡œë“œ (ë‚´ë¶€ìš©). ì—†ìœ¼ë©´ None ë°˜í™˜"""
    cache_path = _get_cache_path(suffix)
    if os.path.exists(cache_path):
        with open(cache_path, "rb") as f:
            nodes = pickle.load(f)
        logger.info("Nodes loaded from cache: %s (%d nodes)", cache_path, len(nodes))
        return nodes
    logger.info("No cache found for suffix=%r", suffix)
    return None


def save_nodes_cache(cn_nodes, ko_nodes) -> None:
    """CN/KO nodesë¥¼ ê°ê° ìºì‹œ ì €ì¥"""
    _save_cache(cn_nodes, "_cn")
    _save_cache(ko_nodes, "_ko")


def load_nodes_cache() -> tuple:
    """
    CN/KO nodes ìºì‹œ ë¡œë“œ
    
    Returns:
        (cn_nodes, ko_nodes) ë˜ëŠ” (None, None)
    """
    cn_nodes = _load_cache("_cn")
    ko_nodes = _load_cache("_ko")
    if cn_nodes is None or ko_nodes is None:
        return None, None
    return cn_nodes, ko_nodes


# core/init_document_node.py í•˜ë‹¨ì— ì¶”ê°€

def save_nodes_cache_incremental(cn_nodes, ko_nodes) -> None:
    """
    CN/KO nodesë¥¼ ê¸°ì¡´ ìºì‹œì— ë³‘í•©í•˜ì—¬ ì €ì¥ (ì¦ë¶„ ì—…ë°ì´íŠ¸)
    - ê°™ì€ node_idëŠ” ë®ì–´ì“°ê¸° (ì—…ë°ì´íŠ¸)
    - ìƒˆë¡œìš´ node_idëŠ” ì¶”ê°€
    
    Args:
        cn_nodes: ìƒˆë¡œ ì¶”ê°€í•  CN nodes
        ko_nodes: ìƒˆë¡œ ì¶”ê°€í•  KO nodes
    
    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
        - ì‹ ê·œ CSV íŒŒì¼ ì¶”ê°€ ì‹œ
        - ê¸°ì¡´ CSV íŒŒì¼ ìˆ˜ì • ì‹œ
    """
    # ê¸°ì¡´ ìºì‹œ ë¡œë“œ
    existing_cn, existing_ko = load_nodes_cache()
    
    if existing_cn and existing_ko:
        logger.info("Merging with existing cache: cn=%d, ko=%d", len(existing_cn), len(existing_ko))
        
        # dictë¡œ ë³€í™˜í•˜ì—¬ ë³‘í•© (node_id ê¸°ì¤€)
        cn_dict = {n.node_id: n for n in existing_cn}
        ko_dict = {n.node_id: n for n in existing_ko}
        
        # ìƒˆ nodes ì¶”ê°€/ì—…ë°ì´íŠ¸
        updated_cn = 0
        added_cn = 0
        for n in cn_nodes:
            if n.node_id in cn_dict:
                updated_cn += 1
            else:
                added_cn += 1
            cn_dict[n.node_id] = n
        
        updated_ko = 0
        added_ko = 0
        for n in ko_nodes:
            if n.node_id in ko_dict:
                updated_ko += 1
            else:
                added_ko += 1
            ko_dict[n.node_id] = n
        
        # ìºì‹œ ì €ì¥
        _save_cache(list(cn_dict.values()), "_cn")
        _save_cache(list(ko_dict.values()), "_ko")
        
        logger.info(
            "Cache updated: CN (added=%d, updated=%d, total=%d), KO (added=%d, updated=%d, total=%d)",
            added_cn, updated_cn, len(cn_dict),
            added_ko, updated_ko, len(ko_dict)
        )
        print(f"ğŸ“¦ ìºì‹œ ì—…ë°ì´íŠ¸:")
        print(f"  - CN: ì¶”ê°€ {added_cn}ê°œ, ì—…ë°ì´íŠ¸ {updated_cn}ê°œ, ì´ {len(cn_dict)}ê°œ")
        print(f"  - KO: ì¶”ê°€ {added_ko}ê°œ, ì—…ë°ì´íŠ¸ {updated_ko}ê°œ, ì´ {len(ko_dict)}ê°œ")
    else:
        # ê¸°ì¡´ ìºì‹œ ì—†ìœ¼ë©´ ìƒˆë¡œ ì €ì¥
        logger.info("No existing cache, creating new cache")
        _save_cache(cn_nodes, "_cn")
        _save_cache(ko_nodes, "_ko")
        print(f"ğŸ“¦ ìƒˆ ìºì‹œ ìƒì„±: CN {len(cn_nodes)}ê°œ, KO {len(ko_nodes)}ê°œ")
