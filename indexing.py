# indexing.py
# ì¸ë±ì‹± ê´€ë ¨ í•¨ìˆ˜ ì •ì˜
import logging
import chromadb
import glob
import pandas as pd
from pathlib import Path
from tqdm import tqdm

from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import VectorStoreIndex, StorageContext, Settings
from core.init_document_node import (
    load_dataset,
    build_documents,
    build_nodes,
    save_nodes_cache,
    save_nodes_cache_incremental
)
from core.chroma_utils import add_user_entry_to_glossary
from core.embedding_model import CompassEmbeddingModel


logger = logging.getLogger("translator")

# Glossary ë””ë ‰í† ë¦¬ ê²½ë¡œ
GLOSSARY_DIR = "glossary"


def load_all_csvs_from_directory(directory: str = GLOSSARY_DIR) -> pd.DataFrame:
    """
    ì§€ì •ëœ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  CSV íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ë³‘í•©
    - ì¤‘ë³µ ì œê±° (cn ê¸°ì¤€)
    - ìˆœì„œ ìœ ì§€
    
    Args:
        directory: CSV íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
    
    Returns:
        ë³‘í•©ëœ DataFrame
    
    Raises:
        ValueError: CSV íŒŒì¼ì´ ì—†ê±°ë‚˜ ë¡œë“œ ì‹¤íŒ¨ ì‹œ
    """
    csv_files = sorted(glob.glob(f"{directory}/*.csv"))
    
    if not csv_files:
        raise ValueError(f"No CSV files found in {directory}")
    
    logger.info(f"Found {len(csv_files)} CSV files in {directory}")
    
    all_dfs = []
    for csv_path in csv_files:
        try:
            df = load_dataset(csv_path)
            all_dfs.append(df)
            logger.info(f"  âœ“ {Path(csv_path).name}: {len(df)} rows")
            print(f"  âœ“ {Path(csv_path).name}: {len(df):,}ê°œ í–‰")
        except Exception as e:
            logger.error(f"  âœ— Failed to load {Path(csv_path).name}: {e}")
            print(f"  âœ— {Path(csv_path).name}: ë¡œë“œ ì‹¤íŒ¨ ({e})")
            # ì—ëŸ¬ ë°œìƒí•´ë„ ë‹¤ë¥¸ íŒŒì¼ì€ ê³„ì† ì²˜ë¦¬
    
    if not all_dfs:
        raise ValueError(f"No valid CSV files loaded from {directory}")
    
    # ë³‘í•©
    merged = pd.concat(all_dfs, ignore_index=True)
    before_count = len(merged)
    
    # ì¤‘ë³µ ì œê±° (cn ê¸°ì¤€, ì²« ë²ˆì§¸ í•­ëª© ìœ ì§€)
    merged = merged.drop_duplicates(subset="cn", keep="first")
    after_count = len(merged)
    
    duplicates_removed = before_count - after_count
    if duplicates_removed > 0:
        logger.info(f"Removed {duplicates_removed} duplicate entries")
        print(f"  â†’ ì¤‘ë³µ ì œê±°: {duplicates_removed:,}ê°œ")
    
    logger.info(f"Total merged rows: {after_count}")
    
    return merged

# -----------------------------
# Chroma ì´ˆê¸°í™”
# -----------------------------

def _init_single_collection(
    persist_dir: str, 
    collection_name: str, 
    client: chromadb.PersistentClient = None
) -> ChromaVectorStore:
    """
    ë‹¨ì¼ Chroma ì»¬ë ‰ì…˜ ì´ˆê¸°í™” (ë‚´ë¶€ìš©)
    """
    if client is None:
        client = chromadb.PersistentClient(path=persist_dir)
    collection = client.get_or_create_collection(name=collection_name)
    return ChromaVectorStore(chroma_collection=collection)


def init_chroma(persist_dir: str, clear: bool = False) -> tuple[ChromaVectorStore, ChromaVectorStore]:
    """
    CN/KO ë³„ë„ ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
    
    Args:
        persist_dir: ì €ì¥ ë””ë ‰í† ë¦¬
        clear: Trueë©´ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±
    
    Returns:
        (cn_vector_store, ko_vector_store)
    """
    # ë‹¨ì¼ í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ìºì‹± ë¬¸ì œ ë°©ì§€)
    client = chromadb.PersistentClient(path=persist_dir)
    
    if clear:
        # ë™ì¼í•œ í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì‚­ì œ í›„ ì¬ìƒì„±
        for name in ["game-translation-cn", "game-translation-ko", "game-translation"]:
            try:
                client.delete_collection(name=name)
                logger.info("[clear] Deleted collection '%s'", name)
            except Exception:
                logger.info("[clear] No collection '%s' to delete", name)
    
    cn_store = _init_single_collection(persist_dir, "game-translation-cn", client)
    ko_store = _init_single_collection(persist_dir, "game-translation-ko", client)
    return cn_store, ko_store


# -----------------------------
# index ìƒì„±/ë¡œë”©
# -----------------------------

def build_index_from_nodes(nodes, vector_store: ChromaVectorStore) -> VectorStoreIndex:
    """
    Nodesë¥¼ ì‚¬ìš©í•˜ì—¬ VectorStoreIndex ìƒì„±(ìµœì´ˆ ì¸ë±ì‹± ì‹œ ì‚¬ìš©)
    - CompassEmbeddingModelë¡œ API ì‘ë‹µ ê¸°ë°˜ í† í° ìˆ˜ ì¶”ì 
    """
    # í† í° ì¹´ìš´íŠ¸ ë¦¬ì…‹ (ì´ì „ ì¸ë±ì‹±ì—ì„œ ëˆ„ì ëœ ê°’ ì´ˆê¸°í™”)
    embed_model = Settings.embed_model
    if isinstance(embed_model, CompassEmbeddingModel):
        embed_model.reset_token_count()
    
    storage = StorageContext.from_defaults(vector_store=vector_store)
    
    # === ì§„í–‰ë„ í‘œì‹œ ===
    # ================================================
    print(f"\nğŸ”„ ì„ë² ë”© ì‹œì‘: ì´ {len(nodes):,}ê°œ ë…¸ë“œ")
    print(f"   (ë°°ì¹˜ í¬ê¸°: {getattr(embed_model, 'embed_batch_size', 'N/A')}ê°œì”© ì²˜ë¦¬)")
    
    # ì„ë² ë”© ì§„í–‰ë„ í‘œì‹œ
    with tqdm(total=len(nodes), desc="ğŸ“ ì„ë² ë”©", unit="nodes", ncols=80) as pbar:
        # VectorStoreIndex ìƒì„± ì‹œ ë‚´ë¶€ì ìœ¼ë¡œ embed_batch_size ë‹¨ìœ„ë¡œ ì„ë² ë”© ì§„í–‰
        index = VectorStoreIndex(
            nodes=nodes, 
            storage_context=storage,
            show_progress=False  # LlamaIndex ë‚´ë¶€ ì§„í–‰ë„ëŠ” ë„ê³  ìˆ˜ë™ ì œì–´
        )
        pbar.update(len(nodes))  # ì™„ë£Œ ì‹œ ì „ì²´ ì—…ë°ì´íŠ¸
    # ================================================
    
    # API ì‘ë‹µ ê¸°ë°˜ ì •í™•í•œ í† í° ìˆ˜ ë¡œê¹…
    if isinstance(embed_model, CompassEmbeddingModel):
        logger.info(
            "Index created: nodes=%d, embedding_tokens=%d (from API)",
            len(nodes),
            embed_model.total_tokens
        )
        print(f"âœ… ì„ë² ë”© ì™„ë£Œ: {len(nodes):,}ê°œ ë…¸ë“œ, {embed_model.total_tokens:,} í† í°")        
    else:
        logger.info("Index created: nodes=%d", len(nodes))
        print(f"âœ… ì„ë² ë”© ì™„ë£Œ: {len(nodes):,}ê°œ ë…¸ë“œ")        
    
    return index


def run_indexing(csv_path: str = None, clear: bool = False):
    """
    CN/KO ë³„ë„ ì»¬ë ‰ì…˜ìœ¼ë¡œ ì¸ë±ì‹±
    
    1. CSVì—ì„œ ë°ì´í„° ë¡œë“œ
    2. CN/KO Documents ë¶„ë¦¬ ìƒì„±
    3. CN/KO ê°ê° Node ìƒì„±
    4. CN/KO ë³„ë„ ì»¬ë ‰ì…˜ì— ì¸ë±ì‹±
    5. CN/KO ë³„ë„ ìºì‹œ ì €ì¥
    
    Args:
        csv_path: CSV íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ glossary í´ë”ì˜ ëª¨ë“  CSV íŒŒì¼)
        clear: Trueë©´ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ìƒˆë¡œ ì¸ë±ì‹±
    
    ì‚¬ìš©ì˜ˆì‹œ:
        # ëª¨ë“  CSV íŒŒì¼ ì¸ë±ì‹±(ì „ì²´ ReIndexing)
        run_indexing(clear=True)
        # íŠ¹ì • íŒŒì¼ë§Œ ì¦ë¶„ ì¸ë±ì‹±
        run_indexing(csv_path="glossary/new_terms.csv", clear=False)
    """
    print("\n" + "="*60)
    print("ğŸ“š Glossary ì¸ë±ì‹± ì‹œì‘")
    print("="*60)
    
    logger.info("[indexing] start. csv=%s, clear=%s", csv_path, clear)
    
    persist_dir = Settings.env["PERSIST_DIR"]
    
    # 1. CSVíŒŒì¼ ê²°ì • í›„ ë°ì´í„° ë¡œë“œ
    if csv_path is None:
        # ëª¨ë“  CSVíŒŒì¼ ìë™ ë¡œë“œ(Default action)
        logger.info("[indexing] Loading all CSV files from %s", GLOSSARY_DIR)
        csv_files = sorted(glob.glob(f"{GLOSSARY_DIR}/*.csv"))
        print(f"\n[1/7] CSV íŒŒì¼ ê²€ìƒ‰: {len(csv_files)}ê°œ íŒŒì¼ ë°œê²¬")
        for f in csv_files:
            print(f"  - {Path(f).name}")
        
        print("\n[2/7] CSV ë¡œë“œ ë° ë³‘í•© ì¤‘...")
        df = load_all_csvs_from_directory(GLOSSARY_DIR)
        print(f"âœ… {len(df):,}ê°œ í–‰ ë¡œë“œ ì™„ë£Œ(ì¤‘ë³µ ì œê±° ì™„ë£Œ)")
        
    else:
        # ë‹¨ì¼ íŒŒì¼ ë¡œë“œ
        logger.info("[indexing] Loading single CSV file: %s", csv_path)
        print(f"\n[1/7] CSV ë¡œë“œ ì¤‘... ({Path(csv_path).name})")
        df = load_dataset(csv_path)
        print(f"âœ… {len(df):,}ê°œ í–‰ ë¡œë“œ ì™„ë£Œ")
        print("\n[2/7] ë‹¨ì¼ íŒŒì¼ ëª¨ë“œ (ë³‘í•© ë‹¨ê³„ ê±´ë„ˆëœ€)")

    # 2. Documents ìƒì„±
    print("\n[3/7] Documents ìƒì„± ì¤‘...")
    cn_docs, ko_docs = build_documents(df)
    print(f"âœ… CN: {len(cn_docs):,}ê°œ, KO: {len(ko_docs):,}ê°œ")
    
    # 3. Nodes ìƒì„±
    print("\n[4/7] Nodes ìƒì„± ì¤‘...")
    cn_nodes = build_nodes(cn_docs)
    ko_nodes = build_nodes(ko_docs)
    print(f"âœ… CN: {len(cn_nodes):,}ê°œ, KO: {len(ko_nodes):,}ê°œ")
    
    # CN/KO ë³„ë„ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” (clear ì˜µì…˜ ì „ë‹¬)
    print("\n[5/7] Vector Store ì´ˆê¸°í™” ì¤‘...")
    cn_store, ko_store = init_chroma(persist_dir, clear=clear)
    if clear:
        print("âœ… ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ì¬ìƒì„±")
    else:
        print("âœ… ê¸°ì¡´ ì»¬ë ‰ì…˜ ë¡œë“œ(ì¦ë¶„ Indexing ëª¨ë“œ)")
    
    # 5. CN ì¸ë±ì‹±
    print("\n[6/7] CN ì¸ë±ì‹± ì¤‘...")
    logger.info("[indexing] Indexing CN collection...")
    _ = build_index_from_nodes(cn_nodes, cn_store)
    
    print("\n[6/7] KO ì¸ë±ì‹± ì¤‘...")
    logger.info("[indexing] Indexing KO collection...")
    _ = build_index_from_nodes(ko_nodes, ko_store)
    
    # ìºì‹œ ì €ì¥ (BM25ìš©)
    print("\n[7/7] ìºì‹œ ì €ì¥ ì¤‘...")
    if clear:
        # ì „ì²´ ReIndexing: Cache ë®ì–´ì“°ê¸°
        save_nodes_cache(cn_nodes, ko_nodes)
        print("âœ… Cache ì €ì¥ ì™„ë£Œ (ì „ì²´ ì¬ìƒì„±)")
    else:
        # ì¦ë¶„ Indexing: ê¸°ì¡´ ìºì‹œì— ë³‘í•©
        save_nodes_cache_incremental(cn_nodes, ko_nodes)
        print("âœ… Cache ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì¦ë¶„ Indexing)")

    # === ì™„ë£Œ ===
    print("\n" + "="*60)
    print("ğŸ‰ ì¸ë±ì‹± ì™„ë£Œ!")
    print("="*60)
    print(f"ğŸ“Š í†µê³„:")
    print(f"  - CSV í–‰: {len(df):,}ê°œ")
    print(f"  - CN ë¬¸ì„œ: {len(cn_docs):,}ê°œ â†’ {len(cn_nodes):,}ê°œ ë…¸ë“œ")
    print(f"  - KO ë¬¸ì„œ: {len(ko_docs):,}ê°œ â†’ {len(ko_nodes):,}ê°œ ë…¸ë“œ")
            
    logger.info(
        "[indexing] finished. rows=%d, cn_docs=%d, ko_docs=%d, cn_nodes=%d, ko_nodes=%d, mode=%s",
        len(df), len(cn_docs), len(ko_docs), len(cn_nodes), len(ko_nodes),
        "full" if clear else "incremental"
    )


def run_indexing_for_text(cn: str = "", ko: str = ""):
    """
    ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë¬¸ìì—´ì„ ì¸ë±ì‹± (API/ì›¹ì•±ì—ì„œ ë™ì ìœ¼ë¡œ ìš©ì–´ ì¶”ê°€)
    
    Args:
        cn (str): ì¤‘êµ­ì–´ í…ìŠ¤íŠ¸ (ë‘˜ ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜)
        ko (str): í•œêµ­ì–´ í…ìŠ¤íŠ¸ (ë‘˜ ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜)
    
    ì‚¬ìš© ì˜ˆì‹œ:
        # ì–‘ë°©í–¥
        run_indexing_for_text("è¶…ç°è±¡ç®¡ç†å±€", "ì´ˆí˜„ìƒ ê´€ë¦¬êµ­")
        
        # ì¤‘êµ­ì–´ë§Œ
        run_indexing_for_text(cn="ç®€æ˜“çªç ´æ¨¡å—ç»„")
        
        # í•œêµ­ì–´ë§Œ
        run_indexing_for_text(ko="ì›”ìƒì„ ê°€ë£¨")
    """
    cn = cn.strip() if cn else ""
    ko = ko.strip() if ko else ""
    
    if not cn and not ko:
        logger.warning("[index_text] cn ë˜ëŠ” ko ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤")
        return
    
    # add_user_entry_to_glossary í™œìš©
    add_user_entry_to_glossary(
        cn=cn,
        ko=ko,
        doc_type="text_input",
        src_lang="cn" if cn else "ko",
        reason="manual_index_text"
    )
    
    logger.info("[index_text] Added: cn=%r, ko=%r", cn, ko)
